{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T06:22:32.685290Z",
     "start_time": "2024-12-09T06:22:32.629098100Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel \n",
    "# from torchvision.transforms import Compose, Resize, ToTensor\n",
    "# from PIL import Image\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to load and preprocess the text input\n",
    "# def preprocess_text(text):\n",
    "#     # Here we can use a tokenizer or any NLP model to process the text\n",
    "#     # For simplicity, we will just return the text\n",
    "#     return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to generate video from text\n",
    "# def generate_video_from_text(text):\n",
    "#     # Load the CLIP model and processor\n",
    "#     model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "#     processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "#     # Preprocess the text\n",
    "#     processed_text = preprocess_text(text)\n",
    "# # \n",
    "#     # Generate video frames using VideoGPT or any other model\n",
    "#     video_model = VideoGPT.from_pretrained(\"your_video_gpt_model\")\n",
    "    \n",
    "#     # Generate video frames based on the processed text\n",
    "#     video_frames = video_model.generate(processed_text)\n",
    "\n",
    "#     return video_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to save video frames as a video file\n",
    "# def save_video(frames, filename='output_video.mp4'):\n",
    "#     # Convert frames to a video format\n",
    "#     # This is a placeholder for actual video saving logic\n",
    "#     # You can use libraries like OpenCV or imageio to save the frames\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     text_input = \"A beautiful sunset over the mountains\"\n",
    "#     video_frames = generate_video_from_text(text_input)\n",
    "#     save_video(video_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# # Step 1: Data Understanding\n",
    "\n",
    "# def parse_input_text(text):\n",
    "#     # Extract data and statistics from the input text\n",
    "#     return text  # For simplicity, returning the text directly\n",
    "\n",
    "# # Step 2: Text Processing\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     return text.strip()\n",
    "\n",
    "# # Step 3: Video Generation\n",
    "\n",
    "# def generate_video_from_text(text):\n",
    "#     # Preprocess the input text\n",
    "#     processed_text = preprocess_text(text)\n",
    "\n",
    "#     # Load the CLIP model and processor\n",
    "#     model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "#     processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "#     # Since VideoGPT cannot be imported, we will use an alternative approach\n",
    "#     import numpy as np\n",
    "#     import cv2\n",
    "\n",
    "#     # Generate a simple video with random colors as a placeholder\n",
    "#     height, width, num_frames = 240, 320, 30\n",
    "#     video_frames = []\n",
    "\n",
    "#     for _ in range(num_frames):\n",
    "#         # Create a random color frame\n",
    "#         frame = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n",
    "#         video_frames.append(frame)\n",
    "\n",
    "#     # Convert list of frames to a video file\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (width, height))\n",
    "\n",
    "#     for frame in video_frames:\n",
    "#         out.write(frame)\n",
    "\n",
    "#     out.release()\n",
    "\n",
    "# # Main function\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example input text\n",
    "#     input_text = \"20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\"\n",
    "    \n",
    "#     # Parse the input text\n",
    "#     try:\n",
    "#         data = parse_input_text(input_text)\n",
    "#         print(\"Parsed Data:\", data)\n",
    "\n",
    "#         # Generate video frames from text\n",
    "#         video_frames = generate_video_from_text(input_text)\n",
    "#         if not video_frames:\n",
    "#             print(\"No suitable video frames generated. Please check the input data or model response.\")\n",
    "#         else:\n",
    "#             print(\"Generated Video Frames:\", video_frames)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It seems you are looking for a model that can generate videos from text. One of the popular models for text-to-video generation is the \"Make-A-Video\" model by Meta. \n",
    "# # However, if you are facing issues with existing models, you might want to try using the \"Stable Diffusion\" model for generating video frames from text prompts.\n",
    "# # Below is an example of how you can implement a simple text-to-video generation using the Stable Diffusion model.\n",
    "\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_video_from_text(prompt):\n",
    "#     # Load the Stable Diffusion model for video generation\n",
    "#     video_model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "#     video_model = video_model.to(\"cuda\")  # Move model to GPU if available\n",
    "\n",
    "#     # Generate video frames using the Stable Diffusion model\n",
    "#     video_frames = []\n",
    "#     for _ in range(30):  # Generate 30 frames\n",
    "#         frame = video_model(prompt).images[0]  # Generate a single frame\n",
    "#         video_frames.append(frame)\n",
    "\n",
    "#     # Return the generated frames instead of saving them to a video file\n",
    "#     return video_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole API implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python script is a program that interacts with an API to generate a video based on a given prompt using a specific model. Here's a breakdown of what the script does:\n",
    "# # import os\n",
    "# import time\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# api_key = \"\"\n",
    "\n",
    "# prompt = \"cat dancing\"\n",
    "# model = \"video-01\" \n",
    "# output_file_name = \"output.mp4\"  # Please enter the save path for the generated video here\n",
    "\n",
    "# def invoke_video_generation() -> str:\n",
    "#     print(\"-----------------Submit video generation task-----------------\")\n",
    "#     url = \"https://api.minimaxi.chat/v1/video_generation\"\n",
    "#     payload = json.dumps({\n",
    "#         \"prompt\": prompt,\n",
    "#         \"model\": model\n",
    "#     })\n",
    "#     headers = {\n",
    "#         'authorization': 'Bearer ' + api_key,\n",
    "#         'content-type': 'application/json',\n",
    "#     }\n",
    "\n",
    "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "#     print(response.text)\n",
    "    \n",
    "#     # Check for errors in the response\n",
    "#     if response.status_code != 200 or 'task_id' not in response.json():\n",
    "#         print(\"Error: \", response.json().get('base_resp', {}).get('status_msg', 'Unknown error occurred'))\n",
    "#         if response.json().get('base_resp', {}).get('status_code') == 1008:\n",
    "#             print(\"Video generation failed due to insufficient balance.\")\n",
    "#         return \"\"\n",
    "    \n",
    "#     task_id = response.json()['task_id']\n",
    "#     print(\"Video generation task submitted successfully, task ID: \" + task_id)\n",
    "#     return task_id\n",
    "\n",
    "# def query_video_generation(task_id: str):\n",
    "#     url = \"https://api.minimaxi.chat/v1/query/video_generation?task_id=\" + task_id\n",
    "#     headers = {\n",
    "#         'authorization': 'Bearer ' + api_key\n",
    "#     }\n",
    "#     response = requests.request(\"GET\", url, headers=headers)\n",
    "#     status = response.json()['status']\n",
    "    \n",
    "#     if status == 'Queueing':\n",
    "#         print(\"...In the queue...\")\n",
    "#         return \"\", 'Queueing'\n",
    "#     elif status == 'Processing':\n",
    "#         print(\"...Generating...\")\n",
    "#         return \"\", 'Processing'\n",
    "#     elif status == 'Success':\n",
    "#         return response.json()['file_id'], \"Finished\"\n",
    "#     elif status == 'Fail':\n",
    "#         print(\"Video generation failed. Reason: \", response.json().get('base_resp', {}).get('status_msg', 'Unknown error'))\n",
    "#         return \"\", \"Fail\"\n",
    "#     else:\n",
    "#         return \"\", \"Unknown\"\n",
    "\n",
    "# def fetch_video_result(file_id: str):\n",
    "#     print(\"---------------Video generated successfully, downloading now---------------\")\n",
    "#     url = \"https://api.minimaxi.chat/v1/files/retrieve?file_id=\" + file_id\n",
    "#     headers = {\n",
    "#         'authorization': 'Bearer ' + api_key,\n",
    "#     }\n",
    "\n",
    "#     response = requests.request(\"GET\", url, headers=headers)\n",
    "#     print(response.text)\n",
    "\n",
    "#     download_url = response.json()['file']['download_url']\n",
    "#     print(\"Video download link: \" + download_url)\n",
    "#     with open(output_file_name, 'wb') as f:\n",
    "#         f.write(requests.get(download_url).content)\n",
    "#     print(\"The video has been downloaded in: \" + os.getcwd() + '/' + output_file_name)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     task_id = invoke_video_generation()\n",
    "#     print(\"-----------------Video generation task submitted -----------------\")\n",
    "#     while True:\n",
    "#         time.sleep(3)  # Rate limiting to 20 requests per minute (3 seconds per request)\n",
    "\n",
    "#         file_id, status = query_video_generation(task_id)\n",
    "#         if file_id != \"\":\n",
    "#             fetch_video_result(file_id)\n",
    "#             print(\"---------------Successful---------------\")\n",
    "#             break\n",
    "#         elif status == \"Fail\" or status == \"Unknown\":\n",
    "#             print(\"---------------Failed---------------\")\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # pip install runwayml\n",
    "# from runwayml import RunwayML\n",
    "\n",
    "# # The env var RUNWAYML_API_SECRET is expected to contain your API key.\n",
    "# client = RunwayML()\n",
    "\n",
    "# task = client.image_to_video.create(\n",
    "#   model='gen3a_turbo',\n",
    "#   prompt_image='https://example.com/assets/bunny.jpg',\n",
    "#   prompt_text='The bunny is eating a carrot',\n",
    "# )\n",
    "# print(task.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2 Correct implematation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 3 Giving some type error but in use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import moviepy.editor as mpy\n",
    "# import time\n",
    "# import os\n",
    "\n",
    "# # Hardcoded Pexels API key\n",
    "# PEXELS_API_KEY = \"\"  # Replace with your actual Pexels API key\n",
    "\n",
    "# # Function to search for videos using Pexels API\n",
    "# def search_videos(query_string, orientation_landscape=True):\n",
    "#     url = \"https://api.pexels.com/videos/search\"\n",
    "#     headers = {\n",
    "#         \"Authorization\": PEXELS_API_KEY,\n",
    "#         \"User-Agent\": \"Mozilla/5.0\"\n",
    "#     }\n",
    "#     params = {\n",
    "#         \"query\": query_string,\n",
    "#         \"orientation\": \"landscape\" if orientation_landscape else \"portrait\",\n",
    "#         \"per_page\": 15\n",
    "#     }\n",
    "\n",
    "#     response = requests.get(url, headers=headers, params=params)\n",
    "#     return response.json()\n",
    "\n",
    "# # Function to create visualizations based on the type\n",
    "# def create_visualization(data, visualization_type):\n",
    "#     if visualization_type == \"line_chart\":\n",
    "#         plt.plot(data['x'], data['y'].astype(float))  # Ensure y values are float\n",
    "#         plt.title('Growth Over Time')\n",
    "#         plt.xlabel('Time')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid()\n",
    "#         plt.savefig('line_chart.png')\n",
    "#     elif visualization_type == \"bar_chart\":\n",
    "#         plt.bar(data['categories'], data['values'].astype(float))  # Ensure values are float\n",
    "#         plt.title('Distribution of Values')\n",
    "#         plt.xlabel('Categories')\n",
    "#         plt.ylabel('Values')\n",
    "#         plt.savefig('bar_chart.png')\n",
    "#     elif visualization_type == \"pie_chart\":\n",
    "#         plt.pie(data['values'].astype(float), labels=data['categories'], autopct='%1.1f%%')  # Ensure values are float\n",
    "#         plt.title('Comparison of Categories')\n",
    "#         plt.savefig('pie_chart.png')\n",
    "#     plt.close()\n",
    "\n",
    "# # Function to generate video from visualizations\n",
    "# def generate_video_from_visualization():\n",
    "#     # Check if the required image files exist before creating the video\n",
    "#     image_files = ['line_chart.png', 'bar_chart.png', 'pie_chart.png']\n",
    "#     if all(os.path.exists(img) for img in image_files):\n",
    "#         # Ensure all images are the same size\n",
    "#         first_image = mpy.ImageClip(image_files[0])\n",
    "#         width, height = first_image.size\n",
    "#         for img in image_files:\n",
    "#             clip = mpy.ImageClip(img)\n",
    "#             if clip.size != (width, height):\n",
    "#                 print(f\"Image {img} does not match the required size of {width}x{height}. Resizing...\")\n",
    "#                 clip = clip.resize(newsize=(width, height))\n",
    "#                 clip.save_frame(img)  # Save the resized image back to file\n",
    "\n",
    "#         output_video_path = 'data_visualization.mp4'\n",
    "#         clip = mpy.ImageSequenceClip(image_files, fps=1)\n",
    "#         clip.write_videofile(output_video_path)\n",
    "#         print(f\"Video saved at: {output_video_path}\")\n",
    "#     else:\n",
    "#         print(\"One or more visualization files are missing. Please ensure all images are created successfully.\")\n",
    "\n",
    "# # Function to analyze data and suggest visualization methods\n",
    "# def analyze_data(context_text):\n",
    "#     if \"growth\" in context_text or \"increase\" in context_text:\n",
    "#         return \"line_chart\"\n",
    "#     elif \"distribution\" in context_text:\n",
    "#         return \"bar_chart\"\n",
    "#     elif \"comparison\" in context_text:\n",
    "#         return \"pie_chart\"\n",
    "#     else:\n",
    "#         return \"text\"\n",
    "\n",
    "# # Function to generate video using data input\n",
    "# def generate_video_from_text(context_text):\n",
    "#     visualization_type = analyze_data(context_text)\n",
    "#     print(f\"Analyzing context: {context_text}\")\n",
    "    \n",
    "#     # Search for relevant videos using Pexels API\n",
    "#     videos = search_videos(context_text)\n",
    "#     if videos and 'videos' in videos:\n",
    "#         print(f\"Found {len(videos['videos'])} videos for the context.\")\n",
    "    \n",
    "#     # Example data for visualization\n",
    "#     data = {\n",
    "#         'x': np.arange(10),\n",
    "#         'y': np.random.randint(1, 10, size=10).astype(float),  # Ensure y values are float\n",
    "#         'categories': ['A', 'B', 'C', 'D'],\n",
    "#         'values': [15.0, 30.0, 45.0, 10.0]  # Ensure values are float\n",
    "#     }\n",
    "    \n",
    "#     create_visualization(data, visualization_type)\n",
    "#     generate_video_from_visualization()\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == '__main__':\n",
    "#     user_input = \"The growth of sales over the last decade shows a significant increase.\"\n",
    "#     generate_video_from_text(user_input)\n",
    "#     print(\"Video generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import moviepy.editor as mpy\n",
    "# from video.background_video_generator import search_videos, getBestVideo\n",
    "\n",
    "# def create_visualization(data, visualization_type):\n",
    "#     if visualization_type == \"line_chart\":\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.plot(data['x'], data['y'])\n",
    "#         plt.title('Line Chart')\n",
    "#         plt.xlabel('X-axis')\n",
    "#         plt.ylabel('Y-axis')\n",
    "#         plt.savefig('line_chart.png')\n",
    "#         plt.close()\n",
    "#     elif visualization_type == \"bar_chart\":\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.bar(data['categories'], data['values'])\n",
    "#         plt.title('Bar Chart')\n",
    "#         plt.xlabel('Categories')\n",
    "#         plt.ylabel('Values')\n",
    "#         plt.savefig('bar_chart.png')\n",
    "#         plt.close()\n",
    "#     elif visualization_type == \"pie_chart\":\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.pie(data['values'], labels=data['categories'], autopct='%1.1f%%')\n",
    "#         plt.title('Pie Chart')\n",
    "#         plt.savefig('pie_chart.png')\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         print(\"No valid visualization type provided.\")\n",
    "\n",
    "# def generate_video_from_visualization():\n",
    "#     image_files = ['line_chart.png', 'bar_chart.png', 'pie_chart.png']\n",
    "#     if all(os.path.exists(img) for img in image_files):\n",
    "#         output_video_path = 'data_visualization.mp4'\n",
    "#         clip = mpy.ImageSequenceClip(image_files, fps=1)\n",
    "#         clip.write_videofile(output_video_path)\n",
    "#         print(f\"Video saved at: {output_video_path}\")\n",
    "#     else:\n",
    "#         print(\"One or more visualization files are missing. Please ensure all images are created successfully.\")\n",
    "\n",
    "# def generate_video_from_text(context_text):\n",
    "#     visualization_type = analyze_data(context_text)\n",
    "#     print(f\"Analyzing context: {context_text}\")\n",
    "    \n",
    "#     # Search for relevant videos using Pexels API\n",
    "#     videos = search_videos(context_text)\n",
    "#     if videos and 'videos' in videos:\n",
    "#         print(f\"Found {len(videos['videos'])} videos for the context.\")\n",
    "#         video_url = getBestVideo(context_text)\n",
    "#         print(f\"Selected video URL: {video_url}\")\n",
    "    \n",
    "#     # Example data for visualization\n",
    "#     data = {\n",
    "#         'x': np.arange(10),\n",
    "#         'y': np.random.randint(1, 10, size=10),\n",
    "#         'categories': ['A', 'B', 'C', 'D'],\n",
    "#         'values': [15, 30, 45, 10]\n",
    "#     }\n",
    "    \n",
    "#     create_visualization(data, visualization_type)\n",
    "#     generate_video_from_visualization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Processed end to end pipeline code working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import moviepy.editor as mpy\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Hardcoded API key\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PEXELS_API_KEY = os.environ.get('PEXELS_API_KEY')\n",
    "\n",
    "def analyze_data(context_text):\n",
    "    # Simple analysis to determine visualization type based on keywords\n",
    "    if \"percent\" in context_text or \"%\" in context_text:\n",
    "        return \"pie_chart\"\n",
    "    elif \"trend\" in context_text or \"increase\" in context_text or \"decrease\" in context_text:\n",
    "        return \"line_chart\"\n",
    "    else:\n",
    "        return \"bar_chart\"\n",
    "\n",
    "def search_videos(query_string, orientation_landscape=True):\n",
    "    url = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": PEXELS_API_KEY,\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query_string,\n",
    "        \"orientation\": \"landscape\" if orientation_landscape else \"portrait\",\n",
    "        \"per_page\": 15\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    json_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching videos: {json_data.get('message', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(f\"Fetched videos for query '{query_string}': {json_data.get('videos', [])}\")  # Print fetched video data\n",
    "    return json_data\n",
    "\n",
    "def getBestVideo(query_string, orientation_landscape=True, used_vids=[]):\n",
    "    vids = search_videos(query_string, orientation_landscape)\n",
    "    videos = vids.get('videos', [])\n",
    "\n",
    "    if not videos:\n",
    "        print(\"No videos found for the given query.\")\n",
    "        return None\n",
    "\n",
    "    if orientation_landscape:\n",
    "        filtered_videos = [video for video in videos if video['width'] >= 1920 and video['height'] >= 1080]\n",
    "    else:\n",
    "        filtered_videos = [video for video in videos if video['width'] >= 1080 and video['height'] >= 1920]\n",
    "\n",
    "    sorted_videos = sorted(filtered_videos, key=lambda x: abs(15 - int(x['duration'])))\n",
    "\n",
    "    for video in sorted_videos:\n",
    "        for video_file in video['video_files']:\n",
    "            if orientation_landscape and video_file['width'] == 1920 and video_file['height'] == 1080:\n",
    "                if video_file['link'].split('.hd')[0] not in used_vids:\n",
    "                    print(f\"Found video URL: {video_file['link']}\")  # Print found video URL\n",
    "                    return video_file['link']\n",
    "            elif not orientation_landscape and video_file['width'] == 1080 and video_file['height'] == 1920:\n",
    "                if video_file['link'].split('.hd')[0] not in used_vids:\n",
    "                    print(f\"Found video URL: {video_file['link']}\")  # Print found video URL\n",
    "                    return video_file['link']\n",
    "    print(\"No suitable video found.\")\n",
    "    return None\n",
    "\n",
    "def create_visualization(data, visualization_type):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if visualization_type == \"line_chart\":\n",
    "        plt.plot(data['x'], data['y'])\n",
    "        plt.title('Line Chart')\n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.savefig('line_chart.png')\n",
    "        plt.close()\n",
    "    elif visualization_type == \"bar_chart\":\n",
    "        plt.bar(data['categories'], data['values'])\n",
    "        plt.title('Bar Chart')\n",
    "        plt.xlabel('Categories')\n",
    "        plt.ylabel('Values')\n",
    "        plt.savefig('bar_chart.png')\n",
    "        plt.close()\n",
    "    elif visualization_type == \"pie_chart\":\n",
    "        plt.pie(data['values'], labels=data['categories'], autopct='%1.1f%%')\n",
    "        plt.title('Pie Chart')\n",
    "        plt.savefig('pie_chart.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No valid visualization type provided.\")\n",
    "\n",
    "def generate_video_from_visualization():\n",
    "    image_files = ['line_chart.png', 'bar_chart.png', 'pie_chart.png']\n",
    "    if all(os.path.exists(img) for img in image_files):\n",
    "        output_video_path = 'data_visualization.mp4'\n",
    "        clip = mpy.ImageSequenceClip(image_files, fps=1)\n",
    "        clip.write_videofile(output_video_path)\n",
    "        print(f\"Video saved at: {output_video_path}\")\n",
    "    else:\n",
    "        print(\"One or more visualization files are missing. Please ensure all images are created successfully.\")\n",
    "\n",
    "def generate_video_from_text(context_text):\n",
    "    visualization_type = analyze_data(context_text)\n",
    "    print(f\"Analyzing context: {context_text}\")\n",
    "    \n",
    "    # Search for relevant videos using Pexels API\n",
    "    video_url = getBestVideo(context_text)\n",
    "    if video_url:\n",
    "        print(f\"Selected video URL: {video_url}\")\n",
    "    else:\n",
    "        print(\"No video URL selected.\")\n",
    "    \n",
    "    # Example data for visualization\n",
    "    data = {\n",
    "        'x': np.arange(10),\n",
    "        'y': np.random.randint(1, 10, size=10),\n",
    "        'categories': ['A', 'B', 'C', 'D'],\n",
    "        'values': [15, 30, 45, 10]\n",
    "    }\n",
    "    \n",
    "    create_visualization(data, visualization_type)\n",
    "    generate_video_from_visualization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import moviepy.editor as mpy\n",
    "import os  # Import os to check for file existence\n",
    "\n",
    "def get_video_from_pexels(query, api_key):\n",
    "    url = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"per_page\": 1\n",
    "    }\n",
    "    \n",
    "    # Rate limiting: wait for 1 second between requests\n",
    "    time.sleep(3)\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['videos']:\n",
    "            return data['videos'][0]['video_files'][0]['link']\n",
    "        else:\n",
    "            print(\"No videos found for the given query.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching videos: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "def analyze_data(context_text):\n",
    "    # Enhanced analysis to determine visualization type based on context\n",
    "    if \"percent\" in context_text or \"%\" in context_text:\n",
    "        return \"pie_chart\"\n",
    "    return \"bar_chart\"  # Default to bar chart if no percentage found\n",
    "\n",
    "def create_visualization(data, visualization_type):\n",
    "    if visualization_type == \"pie_chart\":\n",
    "        plt.pie(data['values'], labels=data['categories'], autopct='%1.1f%%')\n",
    "        plt.title('User Distribution by Brand')\n",
    "        plt.savefig('pie_chart.png')  # Save the pie chart\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.bar(data['categories'], data['values'])\n",
    "        plt.title('Bar Chart')\n",
    "        plt.xlabel('Categories')\n",
    "        plt.ylabel('Values')\n",
    "        plt.savefig('bar_chart.png')  # Save the bar chart\n",
    "        plt.close()\n",
    "\n",
    "def process_text_and_generate_video(query, api_key):\n",
    "    # Step 1: Analyze the text to determine visualization type\n",
    "    visualization_type = analyze_data(query)\n",
    "    print(f\"Analyzing context: {query}\")\n",
    "    \n",
    "    # Step 2: Fetch relevant video from Pexels\n",
    "    video_url = get_video_from_pexels(query, api_key)\n",
    "    if video_url:\n",
    "        print(f\"Selected video URL: {video_url}\")\n",
    "    else:\n",
    "        print(\"No video URL available to create the final video.\")\n",
    "    \n",
    "    # Step 3: Generate a placeholder image based on the query\n",
    "    generated_image_path = 'F:\\\\100x_enginners_hackathon_genai\\\\visualization.png'  # Use a placeholder image instead\n",
    "    if not os.path.exists(generated_image_path):\n",
    "        print(f\"Placeholder image not found at: {generated_image_path}. Please ensure it exists.\")\n",
    "        return  # Exit if the placeholder image does not exist\n",
    "    print(f\"Using placeholder image at: {generated_image_path}\")\n",
    "    \n",
    "    # Step 4: Prepare data for visualization based on the input query\n",
    "    data = {\n",
    "        'categories': ['iPhone', 'Samsung', 'Other'],\n",
    "        'values': [20, 50, 30]  # Assuming the rest is 30%\n",
    "    }\n",
    "    \n",
    "    # Step 5: Create the visualization\n",
    "    create_visualization(data, visualization_type)\n",
    "    \n",
    "    # Step 6: Create infographic video\n",
    "    clips = []\n",
    "    if visualization_type == \"pie_chart\":\n",
    "        clips.append(mpy.ImageClip('pie_chart.png').set_duration(5))\n",
    "    else:\n",
    "        clips.append(mpy.ImageClip('bar_chart.png').set_duration(5))\n",
    "    \n",
    "    # Step 7: Add the generated image to the video clips\n",
    "    clips.append(mpy.ImageClip(generated_image_path).set_duration(5))\n",
    "    \n",
    "    # Step 8: Add a short video clip from Pexels if available\n",
    "    if video_url:\n",
    "        clips.append(mpy.VideoFileClip(video_url).subclip(0, 15))  # Use first 15 seconds of the video\n",
    "    \n",
    "    # Step 9: Concatenate clips (do not save the final video)\n",
    "    final_video = mpy.concatenate_videoclips(clips)\n",
    "    print(\"Infographic video created but not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing context: 20 apples and 30 mangoes\n",
      "Error fetching videos: 401 - {\"status\":401,\"code\":\"Unauthorized\"}\n",
      "No video URL available to create the final video.\n",
      "Placeholder image not found at: F:\\100x_enginners_hackathon_genai\\visualization.png. Please ensure it exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv('PEXELS_API_KEY')\n",
    "# groq_api_key=''\n",
    "process_text_and_generate_video('20 apples and 30 mangoes', api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# def generate_video_from_text(prompt, api_key):\n",
    "#     # Load the Hugging Face pipeline for text-to-video generation\n",
    "#     pipe = pipeline(\"text-to-video\", model=\"CompVis/stable-diffusion-v-1-4\", use_auth_token=api_key)\n",
    "    \n",
    "#     # Generate a video from the text prompt\n",
    "#     video = pipe(prompt)\n",
    "    \n",
    "#     # Save the generated video\n",
    "#     video_path = 'generated_video.mp4'\n",
    "#     video.save(video_path)\n",
    "    \n",
    "#     print(f\"Generated video saved at: {video_path}\")\n",
    "    \n",
    "#     return video_path\n",
    "\n",
    "# # Example usage\n",
    "# generated_video_path = generate_video_from_text(\"A beautiful sunset over the mountains\", api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code need to check , having API faliure 400 error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Data: 20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\n",
      "An error occurred: Pexels API request failed with status code 400\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from groq import Groq\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# import requests  # Ensure requests is imported\n",
    "\n",
    "# # Hardcoded API keys\n",
    "\n",
    "# client = Groq(\n",
    "#     api_key=GROQ_API_KEY,\n",
    "# )\n",
    "\n",
    "# # Step 1: Data Understanding\n",
    "\n",
    "# def parse_input_text(text):\n",
    "#     return text  # For simplicity, returning the text directly\n",
    "\n",
    "# # Step 2: Text Processing\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     return text.strip()\n",
    "\n",
    "# # Step 3: Video Generation\n",
    "\n",
    "# def generate_video_from_text(text):\n",
    "#     # Process the input text to extract relevant data points\n",
    "#     processed_text = preprocess_text(text)\n",
    "\n",
    "#     # Generate a prompt using Groq API based on the processed text\n",
    "#     chat_completion = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"you are a helpful assistant.\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f\"Extract relevant data points from the following text: {processed_text}\",\n",
    "#             }\n",
    "#         ],\n",
    "#         model=\"llama3-8b-8192\",\n",
    "#     )\n",
    "\n",
    "#     prompt = chat_completion.choices[0].message.content\n",
    "\n",
    "#     # Use Pexels API to generate infographic videos\n",
    "#     headers = {'Authorization': PEXELS_API_KEY}\n",
    "#     params = {'query': f'infographics {prompt}', 'per_page': 5}\n",
    "    \n",
    "#     # Rate limiting to avoid exhausting the API\n",
    "#     time.sleep(3)  # Wait for 3 seconds between requests\n",
    "#     response = requests.get('https://api.pexels.com/videos/search', headers=headers, params=params)\n",
    "    \n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(f\"Pexels API request failed with status code {response.status_code}\")\n",
    "\n",
    "#     videos = response.json().get('videos', [])\n",
    "#     video_urls = [video['video_files'][0]['link'] for video in videos if video['video_files']]\n",
    "    \n",
    "#     # Additional rate limiting before returning the URLs\n",
    "#     time.sleep(3)  # Wait for 2 seconds before returning the video URLs\n",
    "#     return video_urls[:3]  # Return only the first 3 video URLs\n",
    "\n",
    "# # Main function\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_text = \"20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\"\n",
    "    \n",
    "#     try:\n",
    "#         data = parse_input_text(input_text)\n",
    "#         print(\"Parsed Data:\", data)\n",
    "\n",
    "#         video_frames = generate_video_from_text(input_text)\n",
    "#         print(\"Generated Video URLs:\", video_frames)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# from transformers import pipeline\n",
    "\n",
    "# def generate_video_with_ltx_model(text):\n",
    "#     # Load the Lightricks/LTX-Video model\n",
    "#     video_generator = pipeline(\"text-to-video\", model=\"Lightricks/LTX-Video\")\n",
    "    \n",
    "#     # Generate video from the input text\n",
    "#     video_output = video_generator(text)\n",
    "    \n",
    "#     return video_output\n",
    "\n",
    "# # Example usage\n",
    "# input_text = \"20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\"\n",
    "# video_output = generate_video_with_ltx_model(input_text)\n",
    "# print(\"Generated Video Output:\", video_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import DiffusionPipeline\n",
    "# from langchain import LLMChain\n",
    "\n",
    "# def generate_video_with_combined_models(text):\n",
    "#     try:\n",
    "#         # Load the models using DiffusionPipeline\n",
    "#         ltx_video_generator = DiffusionPipeline.from_pretrained(\"Lightricks/LTX-Video\")\n",
    "#         mochi_video_generator = DiffusionPipeline.from_pretrained(\"genmo/mochi-1-preview\")\n",
    "\n",
    "#         # Generate video outputs from both models\n",
    "#         ltx_output = ltx_video_generator(text).images[0]\n",
    "#         mochi_output = mochi_video_generator(text).images[0]\n",
    "\n",
    "#         return {\"mochi_output\": mochi_output, \"ltx_output\": ltx_output}  # Return outputs as a dictionary\n",
    "#     except Exception as e:\n",
    "#         print(\"An error occurred while generating video: \", str(e))\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# input_text = \"20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\"\n",
    "# video_output = generate_video_with_combined_models(input_text)\n",
    "# if video_output:\n",
    "#     print(\"Generated Video Outputs:\", video_output)\n",
    "# else:\n",
    "#     print(\"Video generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import moviepy.editor as mp\n",
    "\n",
    "# def fetch_videos_from_pexels(prompt, api_key):\n",
    "#     url = \"https://api.pexels.com/videos/search\"\n",
    "#     headers = {\n",
    "#         \"Authorization\": api_key\n",
    "#     }\n",
    "#     params = {\n",
    "#         \"query\": prompt,\n",
    "#         \"per_page\": 5  # Fetch a limited number of videos\n",
    "#     }\n",
    "#     response = requests.get(url, headers=headers, params=params)\n",
    "#     if response.status_code == 200:\n",
    "#         return [video['video_files'][0]['link'] for video in response.json()['videos'] if video['video_files']]\n",
    "#     else:\n",
    "#         print(\"Error fetching videos:\", response.status_code)\n",
    "#         return []\n",
    "\n",
    "# def generate_video_from_clips(video_urls, output_filename):\n",
    "#     clips = []\n",
    "#     for url in video_urls:\n",
    "#         try:\n",
    "#             clip = mp.VideoFileClip(url)\n",
    "#             clips.append(clip)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading video from {url}: {e}\")\n",
    "\n",
    "#     if clips:  # Check if there are any valid clips\n",
    "#         final_video = mp.concatenate_videoclips(clips)\n",
    "#         if final_video.duration is not None:  # Ensure duration is set\n",
    "#             final_video.write_videofile(output_filename)\n",
    "#         else:\n",
    "#             print(\"Final video has no duration, cannot write to file.\")\n",
    "#     else:\n",
    "#         print(\"No valid video clips to concatenate.\")\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "# input_prompt = \"nature scenery\"\n",
    "# video_urls = fetch_videos_from_pexels(input_prompt, api_key)\n",
    "# if video_urls:\n",
    "#     generate_video_from_clips(video_urls, \"output_video.mp4\")\n",
    "# else:\n",
    "#     print(\"No videos found for the given prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# from transformers import pipeline\n",
    "\n",
    "# def generate_video_with_ltx_model(text):\n",
    "#     # Load the Lightricks/LTX-Video model\n",
    "#     video_generator = pipeline(\"text-to-video\", model=\"Lightricks/LTX-Video\")\n",
    "    \n",
    "#     # Generate video from the input text\n",
    "#     video_output = video_generator(text)\n",
    "    \n",
    "#     return video_output\n",
    "\n",
    "# # Example usage\n",
    "# input_text = \"20% of users own an iPhone, 50% own a Samsung, and the rest own a variety of brands\"\n",
    "# video_output = generate_video_with_ltx_model(input_text)\n",
    "# print(\"Generated Video Output:\", video_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample code impkemtaion using different tecnhique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import spacy\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Load the spaCy model for NLP tasks\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def analyze_text(input_text):\n",
    "#     # Analyze the input text using spaCy\n",
    "#     doc = nlp(input_text)\n",
    "    \n",
    "#     # Extract keywords and entities\n",
    "#     keywords = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "#     entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "#     return keywords, entities\n",
    "\n",
    "# def generate_video_from_text(input_text, api_key):\n",
    "#     # Step 1: Analyze the input text\n",
    "#     keywords, entities = analyze_text(input_text)\n",
    "#     print(\"Keywords:\", keywords)\n",
    "#     print(\"Entities:\", entities)\n",
    "    \n",
    "#     # Step 2: Fetch video clips based on keywords\n",
    "#     video_urls = []\n",
    "#     for keyword in keywords:\n",
    "#         video_urls.extend(fetch_videos_from_pexels(keyword, api_key))\n",
    "    \n",
    "#     # Step 3: Generate video from fetched clips\n",
    "#     if video_urls:\n",
    "#         generate_video_from_clips(video_urls, \"output_video.mp4\")\n",
    "#     else:\n",
    "#         print(\"No videos found for the given keywords.\")\n",
    "\n",
    "# # Example usage\n",
    "# input_text = \"A beautiful sunset over the mountains\"\n",
    "# generate_video_from_text(input_text, api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from moviepy.editor import ImageClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "# import re\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# # Step 1: Text Input Handling\n",
    "# def get_text_input():\n",
    "#     # Simulating user input for demonstration purposes\n",
    "#     return \"\"\"\n",
    "#     Sales Data:\n",
    "#     Product A: 150\n",
    "#     Product B: 200\n",
    "#     Product C: 100\n",
    "#     \"\"\"\n",
    "\n",
    "# # Step 2: Text Analysis and Data Extraction\n",
    "# def extract_data_from_text(text):\n",
    "#     data = {}\n",
    "#     lines = text.strip().split('\\n')\n",
    "#     for line in lines:\n",
    "#         match = re.match(r'(\\w+):\\s*(\\d+)', line)\n",
    "#         if match:\n",
    "#             label, value = match.groups()\n",
    "#             data[label] = int(value)\n",
    "#     return data\n",
    "\n",
    "# # Step 3: Data Visualization\n",
    "# def create_visualization(data):\n",
    "#     # Create a DataFrame using pandas\n",
    "#     df = pd.DataFrame(list(data.items()), columns=['Product', 'Sales'])\n",
    "\n",
    "#     # Create a figure and axis for the plot\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#     # Create a bar chart for sales data\n",
    "#     bars = ax.bar(df['Product'], df['Sales'], color='skyblue')\n",
    "\n",
    "#     # Add data labels on top of each bar\n",
    "#     for bar in bars:\n",
    "#         yval = bar.get_height()\n",
    "#         ax.text(bar.get_x() + bar.get_width()/2, yval, yval, va='bottom', ha='center', fontsize=12)\n",
    "\n",
    "#     # Set labels and title\n",
    "#     ax.set_xlabel('Products', fontsize=14)\n",
    "#     ax.set_ylabel('Sales', fontsize=14)\n",
    "#     ax.set_title('Sales Data Visualization', fontsize=16)\n",
    "\n",
    "#     # Save the visualization as an image\n",
    "#     plt.savefig('bar_chart.png', bbox_inches='tight')  # Save with tight layout\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Create an animated version of the bar chart\n",
    "#     create_animation(df)\n",
    "\n",
    "# def create_animation(df):\n",
    "#     # Create a figure for the animation\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     ax.set_xlim(0, len(df))\n",
    "    \n",
    "#     # Ensure that the maximum sales value is valid before setting ylim\n",
    "#     max_sales = df['Sales'].max() if not df['Sales'].empty else 0\n",
    "#     ax.set_ylim(0, max_sales * 1.1)\n",
    "    \n",
    "#     ax.set_xlabel('Products', fontsize=14)\n",
    "#     ax.set_ylabel('Sales', fontsize=14)\n",
    "#     ax.set_title('Animated Sales Data Visualization', fontsize=16)\n",
    "\n",
    "#     # Create a bar container\n",
    "#     bars = ax.bar(df['Product'], [0]*len(df), color='skyblue')\n",
    "\n",
    "#     # Animation function\n",
    "#     def animate(i):\n",
    "#         for bar, new_height in zip(bars, df['Sales']):\n",
    "#             bar.set_height(new_height * (i / 10))  # Animate to full height over 10 frames\n",
    "#         return bars\n",
    "\n",
    "#     # Create the animation\n",
    "#     ani = FuncAnimation(fig, animate, frames=10, interval=200, blit=True)\n",
    "#     ani.save('animated_bar_chart.gif', writer='pillow', fps=10)  # Save the animation as a GIF instead of MP4\n",
    "\n",
    "# # Step 4: Video Composition\n",
    "# def create_video(image_files, video_urls):\n",
    "#     clips = [ImageClip(img).set_duration(3) for img in image_files]  # Each image displayed for 3 seconds\n",
    "#     video_clips = []\n",
    "    \n",
    "#     for url in video_urls:\n",
    "#         try:\n",
    "#             video_clip = ImageClip(url).set_duration(3)  # Set duration for each video clip\n",
    "#             video_clips.append(video_clip)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading video from {url}: {e}\")\n",
    "\n",
    "#     clips.extend(video_clips)  # Combine image clips with video clips\n",
    "\n",
    "#     if clips:  # Check if there are any clips to concatenate\n",
    "#         final_video = concatenate_videoclips(clips)\n",
    "#         if hasattr(final_video, 'duration') and final_video.duration is not None and final_video.duration > 0:  # Ensure duration is set and positive\n",
    "#             final_video.write_videofile(\"infographic_video.mp4\", codec=\"libx264\")\n",
    "#         else:\n",
    "#             print(\"Final video duration is not set or is zero. Cannot write video file.\")\n",
    "#     else:\n",
    "#         print(\"No clips to create a video.\")\n",
    "\n",
    "# def add_text_overlay(image_file, text):\n",
    "#     image_clip = ImageClip(image_file).set_duration(3)\n",
    "#     text_clip = TextClip(text, fontsize=50, color='white', bg_color='black', size=image_clip.size)\n",
    "#     text_clip = text_clip.set_position('bottom').set_duration(image_clip.duration)\n",
    "#     return CompositeVideoClip([image_clip, text_clip])\n",
    "\n",
    "# # Main Function\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Step 1: Get text input\n",
    "#     text_input = get_text_input()\n",
    "    \n",
    "#     # Step 2: Extract data from text\n",
    "#     extracted_data = extract_data_from_text(text_input)\n",
    "#     print(\"Extracted Data:\", extracted_data)\n",
    "    \n",
    "#     # Step 3: Create visualizations\n",
    "#     create_visualization(extracted_data)\n",
    "    \n",
    "#     # Step 4: Prepare video with text overlay\n",
    "#     image_files = ['bar_chart.png']\n",
    "#     video_urls = []\n",
    "    \n",
    "#     for label in extracted_data.keys():\n",
    "#         overlay_text = f\"{label}: {extracted_data[label]}\"\n",
    "#         video_urls.append(add_text_overlay('bar_chart.png', overlay_text))\n",
    "    \n",
    "#     # Create the final video\n",
    "#     create_video(image_files, video_urls)\n",
    "    \n",
    "#     print(\"Infographic video generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from moviepy.editor import ImageClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "# import re\n",
    "\n",
    "# # Step 1: Text Input Handling\n",
    "# def get_text_input():\n",
    "#     # Simulating user input for demonstration purposes\n",
    "#     return \"\"\"\n",
    "#     Sales Data:\n",
    "#     Product A: 150\n",
    "#     Product B: 200\n",
    "#     Product C: 100\n",
    "#     \"\"\"\n",
    "\n",
    "# # Step 2: Text Analysis and Data Extraction\n",
    "# def extract_data_from_text(text):\n",
    "#     data = {}\n",
    "#     lines = text.strip().split('\\n')\n",
    "#     for line in lines:\n",
    "#         match = re.match(r'(\\w+):\\s*(\\d+)', line)\n",
    "#         if match:\n",
    "#             label, value = match.groups()\n",
    "#             data[label] = int(value)\n",
    "#     return data\n",
    "\n",
    "# # Step 3: Data Visualization\n",
    "# def create_bar_chart(data, title):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(data.keys(), data.values(), color='skyblue')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Products')\n",
    "#     plt.ylabel('Sales')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('bar_chart.png')  # Save the chart as an image\n",
    "#     plt.close()\n",
    "\n",
    "# # Step 4: Video Composition\n",
    "# def create_video(image_files):\n",
    "#     clips = [ImageClip(img).set_duration(3) for img in image_files]  # Each image displayed for 3 seconds\n",
    "#     final_video = concatenate_videoclips(clips)\n",
    "#     final_video.write_videofile(\"infographic_video.mp4\", codec=\"libx264\")\n",
    "\n",
    "# def add_text_overlay(image_file, text):\n",
    "#     image_clip = ImageClip(image_file).set_duration(3)\n",
    "#     text_clip = TextClip(text, fontsize=50, color='white', bg_color='black', size=image_clip.size)\n",
    "#     text_clip = text_clip.set_position('bottom').set_duration(image_clip.duration)\n",
    "#     return CompositeVideoClip([image_clip, text_clip])\n",
    "\n",
    "# # Main Function\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Step 1: Get text input\n",
    "#     text_input = get_text_input()\n",
    "    \n",
    "#     # Step 2: Extract data from text\n",
    "#     extracted_data = extract_data_from_text(text_input)\n",
    "#     print(\"Extracted Data:\", extracted_data)\n",
    "    \n",
    "#     # Step 3: Create visualizations\n",
    "#     create_bar_chart(extracted_data, \"Sales Data Overview\")\n",
    "    \n",
    "#     # Step 4: Prepare video with text overlay\n",
    "#     image_files = ['bar_chart.png']\n",
    "#     video_clips = []\n",
    "    \n",
    "#     # Create a video clip for the bar chart with overlay text\n",
    "#     overlay_text = \"Sales Data Overview\"\n",
    "#     video_clips.append(add_text_overlay('bar_chart.png', overlay_text))\n",
    "    \n",
    "#     # Create the final video\n",
    "#     create_video([clip for clip in video_clips])\n",
    "\n",
    "#     print(\"Infographic video generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code we can go with \n",
    "# Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API Key: None\n",
      "Parsed Data: {'apple': '20 bananas  40 apples'}\n",
      "An error occurred: Pexels API request failed with status code 401\n"
     ]
    }
   ],
   "source": [
    "import requests  # Ensure requests is imported\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import moviepy\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv(\"PEXELS_API_KEY\")\n",
    "print(f\"Your API Key: {api_key}\")\n",
    "\n",
    "\n",
    "# Hardcoded API keys\n",
    "# PEXELS_API_KEY = 'your_api_key'\n",
    "\n",
    "# Step 1: Data Understanding\n",
    "\n",
    "def parse_input_text(text):\n",
    "    # Extract relevant data points from the input text\n",
    "    data_points = {}\n",
    "    parts = text.split(',')\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if '%' in part:\n",
    "            key, value = part.split('%')\n",
    "            data_points[key.strip()] = float(value.strip().replace('%', '')) / 100\n",
    "        else:\n",
    "            # Handle cases where the part does not contain a percentage\n",
    "            try:\n",
    "                # Check if the part contains a valid key-value pair\n",
    "                key_value = part.split(' ', 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    data_points[key.strip()] = value.strip()\n",
    "                else:\n",
    "                    # Attempt to convert to float if it looks like a number\n",
    "                    try:\n",
    "                        data_points[part] = float(part)\n",
    "                    except ValueError:\n",
    "                        print(f\"Could not parse part: '{part}'\")\n",
    "            except ValueError:\n",
    "                print(f\"Could not parse part: '{part}'\")\n",
    "                continue\n",
    "\n",
    "    # Additional processing to convert data points into relevant information\n",
    "    relevant_info = {}\n",
    "    for key, value in data_points.items():\n",
    "        if isinstance(value, float):\n",
    "            relevant_info[key] = f\"{value * 100}%\"\n",
    "        else:\n",
    "            relevant_info[key] = value\n",
    "\n",
    "    return relevant_info  # Return a dictionary of relevant information\n",
    "\n",
    "# Step 2: Text Processing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase for consistency\n",
    "    cleaned_text = text.strip().lower()\n",
    "    \n",
    "    # Remove duplicates and null values\n",
    "    cleaned_text = ' '.join(sorted(set(cleaned_text.split()), key=lambda x: cleaned_text.index(x))) \n",
    "    return cleaned_text  # Return the cleaned text\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Analyze the sentiment of the input text\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Return the sentiment polarity\n",
    "\n",
    "def semantic_segment_transformation(text):\n",
    "    # Convert the given prompt into a contextual prompt\n",
    "    contextual_prompt = f\"Contextual prompt based on: {text}\"\n",
    "\n",
    "    return contextual_prompt  # Return the contextual prompt\n",
    "\n",
    "# # Process the semantic segment transformation text to generate relevant video\n",
    "# contextual_prompt = semantic_segment_transformation(\"give the employer score among the different companies\")\n",
    "# video_frames = generate_video_from_text(contextual_prompt)\n",
    "# print(\"Generated Video URLs:\", video_frames)\n",
    "\n",
    "def generate_video_from_text(text):\n",
    "    # Process the semantic segment transformation text to generate relevant video\n",
    "    contextual_prompt = semantic_segment_transformation(text)\n",
    "\n",
    "\n",
    "    # Preprocess the input text\n",
    "    processed_prompt = preprocess_text(contextual_prompt)\n",
    "\n",
    "    # Generate a prompt for visualization based on the input data\n",
    "    visualization_prompt = f\"Create an animated infographic video showing the distribution of: {processed_prompt}\"\n",
    "\n",
    "    # Use Pexels API to generate infographic videos\n",
    "    headers = {'Authorization': PEXELS_API_KEY}\n",
    "    params = {'query': f'infographics {visualization_prompt}', 'per_page': 5}\n",
    "    \n",
    "    # Rate limiting to avoid exhausting the API\n",
    "    time.sleep(3)  # Wait for 3 seconds between requests\n",
    "    response = requests.get('https://api.pexels.com/videos/search', headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Pexels API request failed with status code {response.status_code}\")\n",
    "\n",
    "    videos = response.json().get('videos', [])\n",
    "    video_urls = [video['video_files'][0]['link'] for video in videos if video['video_files']]\n",
    "    \n",
    "    # Additional rate limiting before returning the URLs\n",
    "    time.sleep(3)  # Wait for 2 seconds before returning the video URLs\n",
    "    return video_urls[:3]  # Return only the first 3 video URLs\n",
    "\n",
    "# Function to stitch numerical data into the video\n",
    "def stitch_numerical_data(video_url, numerical_data):\n",
    "    # Assuming numerical_data is a dictionary with keys as labels and values as numerical values\n",
    "    # This function would require additional logic to integrate with video editing capabilities\n",
    "    # For demonstration, let's assume we have a function to add text overlays to videos\n",
    "    # This function would take the video URL, numerical data, and return a new video URL with the data stitched\n",
    "    # For simplicity, let's assume we're adding a single text overlay with all numerical data\n",
    "    text_overlay = \" | \".join([f\"{key}: {value}\" for key, value in numerical_data.items()])\n",
    "    # This is a placeholder for the actual video editing logic\n",
    "    # new_video_url = add_text_overlay(video_url, text_overlay)\n",
    "    # return new_video_url\n",
    "    print(f\"Text overlay to be added: {text_overlay}\")\n",
    "    return video_url  # Placeholder return\n",
    "\n",
    "# Main function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = \"apple 20 bananas  40 apples\"\n",
    "    \n",
    "    try:\n",
    "        data = parse_input_text(input_text)\n",
    "        print(\"Parsed Data:\", data)\n",
    "\n",
    "        video_frames = generate_video_from_text(input_text)\n",
    "        print(\"Generated Video URLs:\", video_frames)\n",
    "\n",
    "        # Assuming we want to stitch the parsed numerical data into the first video frame\n",
    "        stitched_video_url = stitch_numerical_data(video_frames[0], data)\n",
    "        print(\"Stitched Video URL:\", stitched_video_url)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Approach  - work going on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import moviepy.editor as mpy\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# # Hardcoded Pexels API key\n",
    "# PEXELS_API_KEY = \"\"  # Replace with your actual Pexels API key\n",
    "\n",
    "# class VideoGenerationAgent:\n",
    "#     def __init__(self, api_key):\n",
    "#         self.api_key = api_key\n",
    "\n",
    "#     def search_videos(self, query_string):\n",
    "#         url = \"https://api.pexels.com/videos/search\"\n",
    "#         headers = {\n",
    "#             \"Authorization\": self.api_key,\n",
    "#             \"User-Agent\": \"Mozilla/5.0\"\n",
    "#         }\n",
    "#         params = {\n",
    "#             \"query\": query_string,\n",
    "#             \"orientation\": \"landscape\",\n",
    "#             \"per_page\": 15\n",
    "#         }\n",
    "#         response = requests.get(url, headers=headers, params=params)\n",
    "#         return response.json()\n",
    "\n",
    "#     def analyze_data(self, context_text):\n",
    "#         if \"growth\" in context_text or \"increase\" in context_text:\n",
    "#             return \"line_chart\"\n",
    "#         elif \"distribution\" in context_text:\n",
    "#             return \"bar_chart\"\n",
    "#         elif \"comparison\" in context_text:\n",
    "#             return \"pie_chart\"\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     def create_visualization(self, data, visualization_type):\n",
    "#         if visualization_type == \"line_chart\":\n",
    "#             plt.plot(data['x'], data['y'].astype(float))\n",
    "#             plt.title('Growth Over Time')\n",
    "#             plt.xlabel('Time')\n",
    "#             plt.ylabel('Value')\n",
    "#             plt.grid()\n",
    "#             plt.savefig('line_chart.png')\n",
    "#         elif visualization_type == \"bar_chart\":\n",
    "#             plt.bar(data['categories'], data['values'].astype(float))\n",
    "#             plt.title('Distribution of Values')\n",
    "#             plt.xlabel('Categories')\n",
    "#             plt.ylabel('Values')\n",
    "#             plt.savefig('bar_chart.png')\n",
    "#         elif visualization_type == \"pie_chart\":\n",
    "#             plt.pie(data['values'].astype(float), labels=data['categories'], autopct='%1.1f%%')\n",
    "#             plt.title('Comparison of Categories')\n",
    "#             plt.savefig('pie_chart.png')\n",
    "#         plt.close()\n",
    "\n",
    "#     def generate_video_from_visualization(self):\n",
    "#         image_files = ['line_chart.png', 'bar_chart.png', 'pie_chart.png']\n",
    "#         if all(os.path.exists(img) for img in image_files):\n",
    "#             clip = mpy.ImageSequenceClip(image_files, fps=1)\n",
    "#             output_video_path = 'data_visualization.mp4'\n",
    "#             clip.write_videofile(output_video_path)\n",
    "#             print(f\"Video saved at: {output_video_path}\")\n",
    "#         else:\n",
    "#             print(\"One or more visualization files are missing.\")\n",
    "\n",
    "#     def generate_video_from_text(self, context_text):\n",
    "#         visualization_type = self.analyze_data(context_text)\n",
    "#         print(f\"Analyzing context: {context_text}\")\n",
    "        \n",
    "#         videos = self.search_videos(context_text)\n",
    "#         if videos and 'videos' in videos:\n",
    "#             print(f\"Found {len(videos['videos'])} videos for the context.\")\n",
    "        \n",
    "#         # Example data for visualization\n",
    "#         data = {\n",
    "#             'x': np.arange(10),\n",
    "#             'y': np.random.randint(1, 10, size=10).astype(float),\n",
    "#             'categories': ['A', 'B', 'C', 'D'],\n",
    "#             'values': [15.0, 30.0, 45.0, 10.0]\n",
    "#         }\n",
    "        \n",
    "#         if visualization_type:\n",
    "#             self.create_visualization(data, visualization_type)\n",
    "#             self.generate_video_from_visualization()\n",
    "#         else:\n",
    "#             print(\"No suitable visualization type found.\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == '__main__':\n",
    "#     user_input = \"The growth of sales over the last decade shows a significant increase.\"\n",
    "#     agent = VideoGenerationAgent(PEXELS_API_KEY)\n",
    "#     agent.generate_video_from_text(user_input)\n",
    "#     print(\"Video generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Whole New Approach -- Innovative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract Numerical Data and Contextual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apples': 20, 'bananas': 30}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_data(input_text):\n",
    "    # Regex to find numbers and their associated labels\n",
    "    pattern = r'(\\d+)\\s*(\\w+)'\n",
    "    matches = re.findall(pattern, input_text)\n",
    "    data_dict = {label: int(value) for value, label in matches}\n",
    "    return data_dict\n",
    "\n",
    "input_text = \"20 apples and 30 bananas\"\n",
    "data = extract_data(input_text)\n",
    "print(data)  # Output: {'apples': 20, 'bananas': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25168\\2904178419.py:55: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('bar_chart.png'))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25168\\2904178419.py:59: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('pie_chart.png'))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25168\\2904178419.py:63: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('pictogram.png'))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def create_gif(data):\n",
    "    # Define a function to create a bar chart\n",
    "    def create_bar_chart(data, title, xlabel, ylabel, color):\n",
    "        fig, ax = plt.subplots()\n",
    "        labels = list(data.keys())\n",
    "        values = list(data.values())\n",
    "        \n",
    "        ax.bar(labels, values, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        \n",
    "        # plt.savefig('bar_chart.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Define a function to create a pie chart\n",
    "    def create_pie_chart(data, title):\n",
    "        fig, ax = plt.subplots()\n",
    "        labels = list(data.keys())\n",
    "        sizes = list(data.values())\n",
    "        \n",
    "        ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        # plt.savefig('pie_chart.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Define a function to create a pictogram\n",
    "    def create_pictogram(data, title, xlabel, ylabel):\n",
    "        fig, ax = plt.subplots()\n",
    "        labels = list(data.keys())\n",
    "        values = list(data.values())\n",
    "        \n",
    "        y_pos = np.arange(len(labels))\n",
    "        ax.barh(y_pos, values, color='green')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        # continue\n",
    "        # plt.savefig('pictogram.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create a GIF (you can create multiple frames for a more dynamic effect)\n",
    "    images = []\n",
    "    for i in range(10):  # Example: create 10 frames\n",
    "        # Create a bar chart frame\n",
    "        create_bar_chart(data, 'Fruit Distribution', 'Fruits', 'Quantity', ['red', 'yellow'])\n",
    "        images.append(imageio.imread('bar_chart.png'))\n",
    "        \n",
    "        # Create a pie chart frame\n",
    "        create_pie_chart(data, 'Fruit Distribution')\n",
    "        images.append(imageio.imread('pie_chart.png'))\n",
    "        \n",
    "        # Create a pictogram frame\n",
    "        create_pictogram(data, 'Fruit Distribution', 'Quantity', 'Fruits')\n",
    "        images.append(imageio.imread('pictogram.png'))\n",
    "    \n",
    "    imageio.mimsave('data_visualization.gif', images, duration=0.5)\n",
    "\n",
    "create_gif(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fetch Contextual Video Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://videos.pexels.com/video-files/2351184/2351184-sd_640_360_25fps.mp4', 'https://videos.pexels.com/video-files/5527320/5527320-hd_1080_1920_24fps.mp4', 'https://videos.pexels.com/video-files/2310761/2310761-sd_960_540_30fps.mp4', 'https://videos.pexels.com/video-files/5471308/5471308-sd_960_540_24fps.mp4', 'https://videos.pexels.com/video-files/4481634/4481634-sd_960_540_24fps.mp4', 'https://videos.pexels.com/video-files/2235947/2235947-hd_1280_720_24fps.mp4', 'https://videos.pexels.com/video-files/2882112/2882112-uhd_3840_2160_24fps.mp4', 'https://videos.pexels.com/video-files/2235949/2235949-sd_640_360_24fps.mp4', 'https://videos.pexels.com/video-files/2235950/2235950-sd_640_360_24fps.mp4', 'https://videos.pexels.com/video-files/1792747/1792747-hd_1920_1080_30fps.mp4']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_videos(query):\n",
    "    API_KEY = ''\n",
    "    url = f'https://api.pexels.com/videos/search?query={query}&per_page=5'\n",
    "    headers = {\n",
    "        'Authorization': API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print (\"Http Error:\",errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print (\"Error Connecting:\",errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print (\"Timeout Error:\",errt)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print (\"Something went wrong:\",err)\n",
    "\n",
    "# Fetch videos for apples and bananas\n",
    "video_data_apples = fetch_videos('apples')\n",
    "video_data_bananas = fetch_videos('bananas')\n",
    "\n",
    "# Extract video URLs\n",
    "video_urls = []\n",
    "for video in video_data_apples['videos']:\n",
    "    video_urls.append(video['video_files'][0]['link'])\n",
    "for video in video_data_bananas['videos']:\n",
    "    video_urls.append(video['video_files'][0]['link'])\n",
    "\n",
    "print(video_urls)  # List of video URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy.editor import VideoFileClip, ImageClip, concatenate_videoclips\n",
    "# # from moviepy.editor import \n",
    "# def create_final_video(video_urls):\n",
    "#     clips = []\n",
    "#     urls_not_found = []\n",
    "#     null_values_found = []\n",
    "    \n",
    "#     # Add the GIF as a clip\n",
    "#     gif_clip = ImageClip(\"data_visualization.gif\").set_duration(2)  # Display for 2 seconds\n",
    "#     clips.append(gif_clip)\n",
    "    \n",
    "#     # Add each video clip\n",
    "#     for url in video_urls:\n",
    "#         try:\n",
    "#             video_clip = VideoFileClip(url).subclip(0, 2)  # Use the first 2 seconds of each video\n",
    "#             if video_clip.duration > 0:  # Check if the video has a valid duration\n",
    "#                 clips.append(video_clip)\n",
    "#             else:\n",
    "#                 null_values_found.append(url)\n",
    "#                 print(f\"Warning: Video clip from {url} has no valid duration.\")\n",
    "#         except Exception as e:\n",
    "#             urls_not_found.append(url)\n",
    "#             print(f\"Error loading video from {url}: {e}\")\n",
    "\n",
    "#     if urls_not_found:\n",
    "#         print(f\"URLs not found: {urls_not_found}\")\n",
    "#     if null_values_found:\n",
    "#         print(f\"URLs with null values found: {null_values_found}\")\n",
    "\n",
    "#     if not clips:\n",
    "#         print(\"No valid clips to concatenate.\")\n",
    "#         return\n",
    "\n",
    "#     # Concatenate all clips\n",
    "#     final_video = concatenate_videoclips(clips)\n",
    "    \n",
    "#     # Write the final video to a file\n",
    "#     final_video.write_videofile(\"infographic_video.mp4\", codec=\"libx264\", fps=24)\n",
    "\n",
    "# create_final_video(video_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests  # Ensure requests is imported\n",
    "# from textblob import TextBlob\n",
    "# import spacy\n",
    "# import moviepy\n",
    "# import nltk\n",
    "# import json\n",
    "# import re\n",
    "# import time  # Import time for rate limiting\n",
    "\n",
    "# # Hardcoded API keys\n",
    "# PEXELS_API_KEY = 'yepFtS82dEUPX41sHUOMVzris34EohIYvW8Czo5Z5s6KQ2WHPPe4eIQA'\n",
    "\n",
    "# # Step 1: Data Understanding\n",
    "\n",
    "# def parse_input_text(text):\n",
    "#     # Extract relevant data points from the input text\n",
    "#     data_points = {}\n",
    "#     parts = text.split(',')\n",
    "#     for part in parts:\n",
    "#         part = part.strip()\n",
    "#         if '%' in part:\n",
    "#             key, value = part.split('%')\n",
    "#             data_points[key.strip()] = float(value.strip().replace('%', '')) / 100\n",
    "#         else:\n",
    "#             # Handle cases where the part does not contain a percentage\n",
    "#             try:\n",
    "#                 # Check if the part contains a valid key-value pair\n",
    "#                 key_value = part.split(' ', 1)\n",
    "#                 if len(key_value) == 2:\n",
    "#                     key, value = key_value\n",
    "#                     data_points[key.strip()] = value.strip()\n",
    "#                 else:\n",
    "#                     # Attempt to convert to float if it looks like a number\n",
    "#                     try:\n",
    "#                         data_points[part] = float(part)\n",
    "#                     except ValueError:\n",
    "#                         print(f\"Could not parse part: '{part}'\")\n",
    "#             except ValueError:\n",
    "#                 print(f\"Could not parse part: '{part}'\")\n",
    "#                 continue\n",
    "\n",
    "#     # Additional processing to convert data points into relevant information\n",
    "#     relevant_info = {}\n",
    "#     for key, value in data_points.items():\n",
    "#         if isinstance(value, float):\n",
    "#             relevant_info[key] = f\"{value * 100}%\"\n",
    "#         else:\n",
    "#             relevant_info[key] = value\n",
    "\n",
    "#     return relevant_info  # Return a dictionary of relevant information\n",
    "\n",
    "# # Step 2: Text Processing\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Convert text to lowercase for consistency\n",
    "#     cleaned_text = text.strip().lower()\n",
    "    \n",
    "#     # Remove duplicates and null values\n",
    "#     cleaned_text = ' '.join(sorted(set(cleaned_text.split()), key=lambda x: cleaned_text.index(x))) \n",
    "#     return cleaned_text  # Return the cleaned text\n",
    "\n",
    "\n",
    "# def analyze_sentiment(text):\n",
    "#     # Analyze the sentiment of the input text\n",
    "#     analysis = TextBlob(text)\n",
    "#     return analysis.sentiment.polarity  # Return the sentiment polarity\n",
    "\n",
    "# def semantic_segment_transformation(text):\n",
    "#     # Convert the given prompt into a contextual prompt\n",
    "#     contextual_prompt = f\"Contextual prompt based on: {text}\"\n",
    "\n",
    "#     return contextual_prompt  # Return the contextual prompt\n",
    "\n",
    "# # # Process the semantic segment transformation text to generate relevant video\n",
    "# # contextual_prompt = semantic_segment_transformation(\"give the employer score among the different companies\")\n",
    "# # video_frames = generate_video_from_text(contextual_prompt)\n",
    "# # print(\"Generated Video URLs:\", video_frames)\n",
    "\n",
    "# def generate_video_from_text(text):\n",
    "#     # Process the semantic segment transformation text to generate relevant video\n",
    "#     contextual_prompt = semantic_segment_transformation(text)\n",
    "\n",
    "\n",
    "#     # Preprocess the input text\n",
    "#     processed_prompt = preprocess_text(contextual_prompt)\n",
    "\n",
    "#     # Generate a prompt for visualization based on the input data\n",
    "#     visualization_prompt = f\"Create an animated infographic video showing the distribution of: {processed_prompt}\"\n",
    "\n",
    "#     # Use Pexels API to generate infographic videos\n",
    "#     headers = {'Authorization': PEXELS_API_KEY}\n",
    "#     params = {'query': f'infographics {visualization_prompt}', 'per_page': 5}\n",
    "    \n",
    "#     # Rate limiting to avoid exhausting the API\n",
    "#     time.sleep(3)  # Wait for 3 seconds between requests\n",
    "#     response = requests.get('https://api.pexels.com/videos/search', headers=headers, params=params)\n",
    "    \n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(f\"Pexels API request failed with status code {response.status_code}\")\n",
    "\n",
    "#     videos = response.json().get('videos', [])\n",
    "#     video_urls = [video['video_files'][0]['link'] for video in videos if video['video_files']]\n",
    "    \n",
    "#     # Additional rate limiting before returning the URLs\n",
    "#     time.sleep(3)  # Wait for 2 seconds before returning the video URLs\n",
    "#     return video_urls[:3]  # Return only the first 3 video URLs\n",
    "\n",
    "# # Function to stitch numerical data into the video\n",
    "# def stitch_numerical_data(video_url, numerical_data):\n",
    "#     # Assuming numerical_data is a dictionary with keys as labels and values as numerical values\n",
    "#     # This function would require additional logic to integrate with video editing capabilities\n",
    "#     # For demonstration, let's assume we have a function to add text overlays to videos\n",
    "#     # This function would take the video URL, numerical data, and return a new video URL with the data stitched\n",
    "#     # For simplicity, let's assume we're adding a single text overlay with all numerical data\n",
    "#     text_overlay = \" | \".join([f\"{key}: {value}\" for key, value in numerical_data.items()])\n",
    "#     # This is a placeholder for the actual video editing logic\n",
    "#     # new_video_url = add_text_overlay(video_url, text_overlay)\n",
    "#     # return new_video_url\n",
    "#     print(f\"Text overlay to be added: {text_overlay}\")\n",
    "#     return video_url  # Placeholder return\n",
    "\n",
    "# # Main function\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_text = \"apple 20 bananas  40 apples\"\n",
    "    \n",
    "#     try:\n",
    "#         data = parse_input_text(input_text)\n",
    "#         print(\"Parsed Data:\", data)\n",
    "\n",
    "#         video_frames = generate_video_from_text(input_text)\n",
    "#         print(\"Generated Video URLs:\", video_frames)\n",
    "\n",
    "#         # Assuming we want to stitch the parsed numerical data into the first video frame\n",
    "#         stitched_video_url = stitch_numerical_data(video_frames[0], data)\n",
    "#         print(\"Stitched Video URL:\", stitched_video_url)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certainly! The complete pipeline you are building is designed to take user-provided numerical data and contextual information, process it, and generate an infographics video. Heres a breakdown of the entire pipeline and how each component contributes to the final output:\n",
    "# Overview of the Pipeline\n",
    "# 1. User Input:\n",
    "# The user provides a prompt that includes numerical data and contextual information (e.g., \"20 apples and 30 bananas\").\n",
    "# 2. Input Segmentation:\n",
    "# The input is segmented into meaningful parts. For example, the input \"20 apples and 30 bananas\" is split into two segments: \"20 apples\" and \"30 bananas\". This allows the model to process each part independently.\n",
    "# 3. Feature Extraction:\n",
    "# A Segment Transformer (or a similar model) is used to extract features from each segment. This involves:\n",
    "# Tokenizing the segments.\n",
    "# Passing them through a pre-trained model (like BERT) to obtain embeddings.\n",
    "# Using either the mean of the hidden states or the CLS token representation to summarize the information in each segment.\n",
    "# 4. Data Visualization:\n",
    "# The extracted features are then used to create visual representations of the numerical data. This could involve generating bar charts, pie charts, or other types of visualizations that represent the quantities of the items mentioned (e.g., a bar chart showing 20 apples and 30 bananas).\n",
    "# The visualizations are saved as images or GIFs.\n",
    "# 5. Contextual Video Retrieval:\n",
    "# The contextual information (e.g., \"apples\" and \"bananas\") is used to fetch relevant video content from a video source, such as the Pexels API. This involves:\n",
    "# Making API calls to search for videos related to the extracted keywords.\n",
    "# Collecting the URLs of the videos that match the context.\n",
    "# Stitching the Video:\n",
    "# The visualizations and the retrieved video clips are combined into a final infographics video. This involves:\n",
    "# Loading the visualizations and video clips.\n",
    "# Concatenating them in a meaningful order (e.g., showing the visualization first, followed by relevant video clips).\n",
    "# Writing the final video to a file.\n",
    "# Output:\n",
    "# The final output is an infographics video that visually represents the user-provided data alongside contextual video content. This video can be used for presentations, educational purposes, or social media sharing.\n",
    "# Summary of Each Component's Role\n",
    "# Input Segmentation: Breaks down the user input into manageable parts for easier processing.\n",
    "# Feature Extraction: Captures the semantic meaning of each segment, allowing for better understanding and representation of the data.\n",
    "# Data Visualization: Creates visual elements that represent the numerical data, making it easier for viewers to understand the information at a glance.\n",
    "# Contextual Video Retrieval: Fetches relevant video content that enhances the narrative of the infographics video, providing context and engagement.\n",
    "# Stitching the Video: Combines all elements into a cohesive final product that effectively communicates the intended message.\n",
    "# Conclusion\n",
    "# This pipeline effectively transforms raw user input into a polished infographics video by leveraging NLP techniques, data visualization, and video editing. Each step is crucial for ensuring that the final output is informative, engaging, and visually appealing. By automating this process, you can create high-quality infographics videos that can be generated quickly and efficiently based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Data Visualization\n",
    "# def create_visualization(data_dict, vis_type='bar'):\n",
    "#     labels = list(data_dict.keys())\n",
    "#     values = list(data_dict.values())\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "    \n",
    "#     if vis_type == 'bar':\n",
    "#         plt.bar(labels, values, color='skyblue')\n",
    "#         plt.title('Bar Chart of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'pie':\n",
    "#         plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "#         plt.title('Pie Chart of Data Distribution')\n",
    "#     elif vis_type == 'line':\n",
    "#         plt.plot(labels, values, marker='o', linestyle='-', color='orange')\n",
    "#         plt.title('Line Graph of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'scatter':\n",
    "#         plt.scatter(labels, values, color='red')\n",
    "#         plt.title('Scatter Plot of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'histogram':\n",
    "#         plt.hist(values, bins=10, color='purple', alpha=0.7)\n",
    "#         plt.title('Histogram of Quantities')\n",
    "#         plt.xlabel('Quantity')\n",
    "#         plt.ylabel('Frequency')\n",
    "#     elif vis_type == 'box':\n",
    "#         sns.boxplot(x=labels, y=values)\n",
    "#         plt.title('Box Plot of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'violin':\n",
    "#         sns.violinplot(x=labels, y=values)\n",
    "#         plt.title('Violin Plot of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'heatmap':\n",
    "#         sns.heatmap(np.array(values).reshape(1, -1), annot=True, cmap='coolwarm', xticklabels=labels)\n",
    "#         plt.title('Heatmap of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'area':\n",
    "#         plt.fill_between(labels, values, color='green', alpha=0.5)\n",
    "#         plt.title('Area Chart of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'stacked_bar':\n",
    "#         plt.bar(labels, values, color='blue', bottom=None, cumulative=True)\n",
    "#         plt.title('Stacked Bar Chart of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == '3d_bar':\n",
    "#         fig = plt.figure()\n",
    "#         ax = fig.add_subplot(111, projection='3d')\n",
    "#         y_pos = np.arange(len(labels))\n",
    "#         ax.bar3d(y_pos, 0, np.zeros(len(labels)), 1, 1, values, color='r')\n",
    "#         ax.set_xlabel('Items')\n",
    "#         ax.set_ylabel('Quantity')\n",
    "#         ax.set_zlabel('Frequency')\n",
    "#         plt.title('3D Bar Chart of Data Distribution')\n",
    "#     elif vis_type == 'bubble':\n",
    "#         plt.scatter(labels, values, s=values, color='blue', alpha=0.5)\n",
    "#         plt.title('Bubble Chart of Data Distribution')\n",
    "#         plt.xlabel('Items')\n",
    "#         plt.ylabel('Quantity')\n",
    "#     elif vis_type == 'radar':\n",
    "#         angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "#         values = np.concatenate((values, [values[0]]))\n",
    "#         plt.polar(angles, values, 'o-', linewidth=2)\n",
    "#         plt.fill(angles, values, 'r', alpha=0.25)\n",
    "#         plt.thetagrids(angles * 180/np.pi, labels)\n",
    "#         plt.title('Radar Chart of Data Distribution')\n",
    "#     elif vis_type == 'wordcloud':\n",
    "#         from wordcloud import WordCloud\n",
    "#         wordcloud = WordCloud().generate_from_frequencies(data_dict)\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title('Word Cloud of Data Distribution')\n",
    "#     elif vis_type == 'treemap':\n",
    "#         from squarify import Squarify\n",
    "#         Squarify(sizes=values, label=labels, alpha=0.8).plot(sizes=values, label=labels, color=\"Blues\")\n",
    "#         plt.title('Treemap of Data Distribution')\n",
    "#     elif vis_type == 'sankey':\n",
    "#         from sankey import Sankey\n",
    "#         sankey = Sankey()\n",
    "#         for label, value in data_dict.items():\n",
    "#             sankey.add(label, value)\n",
    "#         sankey.plot()\n",
    "#         plt.title('Sankey Diagram of Data Distribution')\n",
    "#     elif vis_type == 'parallel_coordinates':\n",
    "#         from pandas.plotting import parallel_coordinates\n",
    "#         df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "#         parallel_coordinates(df, 'index')\n",
    "#         plt.title('Parallel Coordinates of Data Distribution')\n",
    "#     elif vis_type == 'interactive':\n",
    "#         import plotly.graph_objects as go\n",
    "#         fig = go.Figure(data=[go.Bar(y=values, x=labels)])\n",
    "#         fig.update_layout(title_text='Interactive Bar Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     elif vis_type == 'radial':\n",
    "#         from math import pi\n",
    "#         categories = labels\n",
    "#         values = values\n",
    "#         N = len(categories)\n",
    "#         angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "#         values += values[:1]\n",
    "#         angles += angles[:1]\n",
    "#         ax = plt.subplot(111, polar=True)\n",
    "#         plt.xticks(angles[:-1], categories, color='grey', size=8)\n",
    "#         ax.plot(angles, values)\n",
    "#         ax.fill(angles, values, 'b', alpha=0.1)\n",
    "#         plt.title('Radial Chart of Data Distribution')\n",
    "#     elif vis_type == 'sunburst':\n",
    "#         import plotly.express as px\n",
    "#         fig = px.sunburst(\n",
    "#             names=labels,\n",
    "#             parents=[\"\"] * len(labels),\n",
    "#             values=values,\n",
    "#         )\n",
    "#         fig.update_layout(title_text='Sunburst Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     elif vis_type == 'funnel':\n",
    "#         from plotly import graph_objects as go\n",
    "#         fig = go.Figure(go.Funnel(\n",
    "#             y = labels,\n",
    "#             x = values,\n",
    "#             textinfo = \"value+percent initial\"))\n",
    "#         fig.update_layout(title_text='Funnel Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     elif vis_type == 'waterfall':\n",
    "#         from plotly import graph_objects as go\n",
    "#         fig = go.Figure(go.Waterfall(\n",
    "#             name = \"20\", orientation = \"v\",\n",
    "#             measure = [\"relative\", \"relative\", \"total\", \"relative\", \"total\", \"relative\", \"total\"],\n",
    "#             x = labels,\n",
    "#             textposition = \"outside\",\n",
    "#             text = values,\n",
    "#             y = values))\n",
    "#         fig.update_layout(title_text='Waterfall Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     elif vis_type == 'gauge':\n",
    "#         from plotly import graph_objects as go\n",
    "#         fig = go.Figure(go.Indicator(\n",
    "#             mode = \"gauge+number+delta\",\n",
    "#             value = values[0],\n",
    "#             domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "#             title = {'text': \"Speed\"},\n",
    "#             delta = {'reference': values[1]},\n",
    "#             gauge = {\n",
    "#                 'axis': {'range': [None, 100]},\n",
    "#                 'bar': {'color': \"darkblue\"},\n",
    "#                 'steps' : [\n",
    "#                     {'range': [0, 30], 'color': \"lightgray\"},\n",
    "#                     {'range': [30, 60], 'color': \"gray\"}],\n",
    "#                 'threshold' : {\n",
    "#                     'line': {'color': \"red\", 'width': 4},\n",
    "#                     'thickness': 0.75,\n",
    "#                     'value': values[2]}}))\n",
    "#         fig.update_layout(title_text='Gauge Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     elif vis_type == 'bullet':\n",
    "#         from plotly import graph_objects as go\n",
    "#         fig = go.Figure()\n",
    "#         fig.add_trace(go.Indicator(\n",
    "#             mode = \"number+gauge+delta\",\n",
    "#             value = values[0],\n",
    "#             domain = {'x': [0.25, 1], 'y': [0.08, 0.25]},\n",
    "#             title = {'text': \"Success\"},\n",
    "#             delta = {'reference': values[1]},\n",
    "#             gauge = {\n",
    "#                 'axis': {'range': [None, 100]},\n",
    "#                 'bar': {'color': \"black\"},\n",
    "#                 'steps' : [\n",
    "#                     {'range': [0, 30], 'color': \"lightgray\"},\n",
    "#                     {'range': [30, 60], 'color': \"gray\"}],\n",
    "#                 'threshold' : {\n",
    "#                     'line': {'color': \"red\", 'width': 4},\n",
    "#                     'thickness': 0.75,\n",
    "#                     'value': values[2]}}))\n",
    "#         fig.update_layout(title_text='Bullet Chart of Data Distribution')\n",
    "#         fig.show()\n",
    "#     # Add more visualizations as needed...\n",
    "\n",
    "#     plt.savefig('data_visualization.png')\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to Create an Infographic Video\n",
    "# 1. Input Text Processing: Extract numerical data and labels from the input text.\n",
    "# 2. Data Visualization: Generate various visualizations based on the extracted data.\n",
    "# 3. Image Generation: Use Stable Diffusion to create images that represent the data visually.\n",
    "# 4. Video Creation: Stitch together the visualizations and images into a cohesive video.\n",
    "# 5. Final Touches: Add titles, transitions, and background music to enhance the infographic.\n",
    "# Detailed Implementation\n",
    "# Below is a structured implementation that includes detailed parameters for each code block. This will help you create a complete infographic video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
